{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Policyholder Information: This includes demographic details such as age,\n",
    "gender, occupation, marital status, and geographical location.\n",
    "2. Claim History: Information regarding past insurance claims, including claim\n",
    "amounts, types of claims (e.g., medical, automobile), frequency of claims, and\n",
    "claim durations.\n",
    "3. Policy Details: Details about the insurance policies held by the policyholders,\n",
    "such as coverage type, policy duration, premium amount, and deductibles.\n",
    "4. Risk Factors: Variables indicating potential risk factors associated with\n",
    "policyholders, such as credit score, driving record (for automobile insurance),\n",
    "health status (for medical insurance), and property characteristics (for home\n",
    "insurance).\n",
    "5. External Factors: Factors external to the policyholders that may influence claim\n",
    "likelihood, such as economic indicators, weather conditions, and regulatory\n",
    "changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Cleaning and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd test \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Function to extract numerical values from torque and power columns\n",
    "def extract_number(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    return float(value.split('Nm@')[0])\n",
    "\n",
    "def extract_power(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    return float(value.split('bhp@')[0])\n",
    "\n",
    "# Extract INT?/FLOAT from String max_power\n",
    "df['rpm'] = df['max_power'].str.extract(r'@(\\d+)rpm').astype(int)\n",
    "df[\"horse_power\"] = df['max_power'].str.extract(r'(\\d+\\.\\d+)').astype(float)\n",
    "\n",
    "# Clean max_torque and max_power columns (Repeated due to code compatability)\n",
    "df['max_torque'] = df['max_torque'].apply(extract_number)\n",
    "df['max_power'] = df['max_power'].apply(extract_power)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Identify boolean columns (those with Yes/No values)\n",
    "boolean_columns = []\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':  # Check if column is string type\n",
    "        if set(df[column].unique()) == {'Yes', 'No'} or set(df[column].unique()) == {'No', 'Yes'}:\n",
    "            boolean_columns.append(column)\n",
    "print(\"Columns containing Yes/No values:\", boolean_columns)\n",
    "\n",
    "# Replace Yes/No with 1/0 for these columns\n",
    "for column in boolean_columns:\n",
    "    df[column] = df[column].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "continuous_features = ['vehicle_age', 'customer_age', \n",
    "                      'displacement', 'turning_radius', 'length', 'width', 'gross_weight',\n",
    "                      'max_torque', 'max_power']  # Added max_torque and max_power here\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[continuous_features] = scaler.fit_transform(df[continuous_features])\n",
    "\n",
    "# Display summary statistics of standardized features\n",
    "print(\"\\nSummary statistics of standardized features:\")\n",
    "print(df[continuous_features].describe())\n",
    "\n",
    "# Define categorical features - these are columns with text or categorical values\n",
    "categorical_features = ['region_code', 'segment', 'model', 'fuel_type', 'engine_type', \n",
    "                       'airbags', 'rear_brakes_type', 'cylinder', 'transmission_type',\n",
    "                       'steering_type']\n",
    "\n",
    "# Encode categorical variables\n",
    "for column in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Display first few rows of processed dataset\n",
    "print(\"\\nFirst few rows of processed dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistics of the processed dataset\n",
    "print(\"\\nProcessed dataset statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('cleaned_insurance_claims.csv', index=False)\n",
    "print(\"\\nCleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Overall Claims Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "claims_dist = df['claim_status'].value_counts()\n",
    "plt.bar(claims_dist.index, claims_dist.values, color=['lightblue', 'lightcoral'])\n",
    "plt.bar(claims_dist.index, claims_dist.values)\n",
    "plt.title('Overall Distribution of Claims')\n",
    "plt.xlabel('Claim Status (0: No Claim, 1: Claim)')\n",
    "plt.ylabel('Count')\n",
    "plt.text(0, claims_dist[0], f'{claims_dist[0]}', ha='center', va='bottom')\n",
    "plt.text(1, claims_dist[1], f'{claims_dist[1]}', ha='center', va='bottom')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks([0, 1])\n",
    "plt.show()\n",
    "\n",
    "# Calculate claim rate percentage\n",
    "claim_rate = (claims_dist[1] / len(df)) * 100\n",
    "print(f\"\\nOverall Claim Rate: {claim_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original data to get segment names\n",
    "original_df = pd.read_csv('data.csv')\n",
    "\n",
    "# Get mapping of encoded values to original segment names\n",
    "segment_mapping = dict(zip(df['segment'].unique(), original_df['segment'].unique()))\n",
    "df['segment'] = df['segment'].map(segment_mapping)\n",
    "\n",
    "# Claims by Vehicle Segment\n",
    "segment_claims = pd.crosstab(df['segment'], df['claim_status'], normalize='index') * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "segment_claims[1].plot(kind='bar', color='lightblue')\n",
    "plt.title('Claim Rate by Vehicle Segment', fontsize=12, pad=15)\n",
    "plt.xlabel('Vehicle Segment')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the claim rates by segment for verification\n",
    "print(\"\\nClaim rates by vehicle segment:\")\n",
    "print(segment_claims[1].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claims by Fuel Type\n",
    "plt.figure(figsize=(10, 6))\n",
    "fuel_claims = pd.crosstab(original_df['fuel_type'], original_df['claim_status'], normalize='index') * 100\n",
    "fuel_claims[1].sort_values(ascending=False).plot(kind='bar', color='lightblue')\n",
    "plt.title('Claim Rate by Fuel Type', fontsize=12, pad=15)\n",
    "plt.xlabel('Fuel Type')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the claim rates by fuel type for verification\n",
    "print(\"\\nClaim rates by fuel type:\")\n",
    "print(fuel_claims[1].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claims by Vehicle Age Groups \n",
    "original_df['vehicle_age_group'] = pd.qcut(original_df['vehicle_age'], q=5, labels=['Very New', 'New', 'Medium', 'Old', 'Very Old'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "age_claims = pd.crosstab(original_df['vehicle_age_group'], original_df['claim_status'], normalize='index') * 100\n",
    "age_claims[1].plot(kind='bar', color='lightblue')\n",
    "plt.title('Claim Rate by Vehicle Age', fontsize=12, pad=15)\n",
    "plt.xlabel('Vehicle Age Group')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the claim rates by vehicle age for verification\n",
    "print(\"\\nClaim rates by vehicle age groups:\")\n",
    "print(age_claims[1].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety Features Impact on Claims\n",
    "safety_features = ['is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors', \n",
    "                 'is_parking_camera', 'is_front_fog_lights', 'is_brake_assist']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "safety_impact = pd.DataFrame()\n",
    "\n",
    "for feature in safety_features:\n",
    "   feature_claims = pd.crosstab(original_df[feature], original_df['claim_status'], normalize='index')[1] * 100\n",
    "   safety_impact[feature] = feature_claims\n",
    "\n",
    "safety_impact.transpose().plot(kind='bar', color=['purple', 'lightblue'])\n",
    "plt.title('Claim Rate by Safety Features', fontsize=12, pad=15)\n",
    "plt.xlabel('Safety Feature')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.legend(['Without Feature', 'With Feature'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify numerical columns that we want to analyze\n",
    "numerical_features = ['vehicle_age', 'customer_age', 'subscription_length', 'max_torque',\n",
    "                      'max_power', 'displacement', 'turning_radius', 'length', 'width', \n",
    "                     'gross_weight', 'claim_status']\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True)\n",
    "\n",
    "plt.title('Correlation Matrix of Numerical Features with Claim Status', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation with claim_status specifically\n",
    "print(\"\\nCorrelation with claim_status:\")\n",
    "claim_correlations = correlation_matrix['claim_status'].sort_values(ascending=False)\n",
    "print(claim_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claims by Region Density\n",
    "plt.figure(figsize=(10, 6))\n",
    "region_claims = pd.crosstab(original_df['region_density'], original_df['claim_status'], normalize='index') * 100\n",
    "region_claims[1].sort_index().plot(kind='bar', color='violet')\n",
    "plt.title('Claim Rate by Region Density', fontsize=12, pad=15)\n",
    "plt.xlabel('Region Density')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the claim rates by region for verification\n",
    "print(\"\\nClaim rates by region density:\")\n",
    "print(region_claims[1].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claims by Region Code\n",
    "plt.figure(figsize=(12, 6))\n",
    "region_code_claims = pd.crosstab(df['region_code'], df['claim_status'], normalize='index') * 100\n",
    "region_code_claims[1].sort_index().plot(kind='bar', color='violet')\n",
    "plt.title('Claim Rate by Region Code', fontsize=12, pad=15)\n",
    "plt.xlabel('Region Code')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the claim rates by region for verification\n",
    "print(\"\\nClaim rates by region density:\")\n",
    "print(region_code_claims[1].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average claim rate for each combination of region_code and region_density\n",
    "region_combined = pd.crosstab([df['region_code'], df['region_density']], \n",
    "                            df['claim_status'], normalize='index') * 100\n",
    "print(\"\\nClaim rates by Region Code and Density:\")\n",
    "print(region_combined[1].sort_values(ascending=False).head(10))\n",
    "\n",
    "# Number of policies in each region\n",
    "plt.figure(figsize=(12, 6))\n",
    "region_size = df['region_code'].value_counts()\n",
    "region_size.plot(kind='bar', color='violet')\n",
    "plt.title('Number of Policies by Region Code', fontsize=12, pad=15)\n",
    "plt.xlabel('Region Code')\n",
    "plt.ylabel('Number of Policies')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nRegional Analysis Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Highest claim rate regions\n",
    "print(\"\\nTop 5 regions by claim rate:\")\n",
    "top_regions = region_code_claims[1].sort_values(ascending=False).head()\n",
    "print(top_regions)\n",
    "\n",
    "# Region density distribution\n",
    "print(\"\\nDistribution of policies across region densities:\")\n",
    "density_dist = df['region_density'].value_counts()\n",
    "print(density_dist)\n",
    "\n",
    "# Chi-square test for independence between region and claims\n",
    "from scipy.stats import chi2_contingency\n",
    "contingency_table = pd.crosstab(df['region_code'], df['claim_status'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "print(\"\\nChi-square test for independence between region and claims:\")\n",
    "print(f\"Chi-square statistic: {chi2:.2f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(\"Interpretation: \" + (\"There is a significant relationship between region and claims\" \n",
    "                          if p_value < 0.05 else \n",
    "                          \"No significant relationship between region and claims\"))\n",
    "\n",
    "# Calculate and print the regional risk factors\n",
    "risk_factors = pd.DataFrame({\n",
    "    'claim_rate': region_code_claims[1],\n",
    "    'policy_count': df['region_code'].value_counts(),\n",
    "    'avg_density': df.groupby('region_code')['region_density'].mean()\n",
    "}).sort_values('claim_rate', ascending=False)\n",
    "\n",
    "print(\"\\nRegional Risk Factor Analysis:\")\n",
    "print(risk_factors.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique original region codes from the original dataset\n",
    "original_df = pd.read_csv('data.csv')\n",
    "\n",
    "# Create a DataFrame showing the mapping\n",
    "mapping_df = pd.DataFrame({\n",
    "    'Original_Region_Code': original_df['region_code'].unique(),\n",
    "    'Standardized_Value': df['region_code'].unique()\n",
    "})\n",
    "\n",
    "# Sort by original region code for better readability\n",
    "mapping_df = mapping_df.sort_values('Original_Region_Code')\n",
    "\n",
    "print(\"Mapping between Original and Standardized Region Codes:\")\n",
    "print(mapping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics for each analysis\n",
    "print(\"\\nSummary of Claims Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTop 3 fuel types by claim rate:\")\n",
    "print(fuel_claims[1].sort_values(ascending=False).head(3))\n",
    "\n",
    "print(\"\\nVehicle age group claim rates:\")\n",
    "print(age_claims[1].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nRegion density claim rates:\")\n",
    "print(region_claims[1].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nSafety features impact on claim rates:\")\n",
    "for feature in safety_features:\n",
    "    feature_impact = pd.crosstab(df[feature], df['claim_status'], normalize='index')[1]\n",
    "    reduction = feature_impact.iloc[0] - feature_impact.iloc[1]\n",
    "    print(f\"{feature}: {'Reduces' if reduction > 0 else 'Increases'} claim rate by {abs(reduction):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Functions for future use\n",
    "\n",
    "def CalculteOptimalClusters(data, max_clusters: int, sample_size: float, columns_names: list = None, print: bool = False):\n",
    "    # Calculates sillhoute score by using KMeans clustering repeatedly and picking the most optimal number of clusters\n",
    "    cluster_range = range(2, max_clusters)\n",
    "    silhouette_scores = []\n",
    "\n",
    "    if columns_names is not None:\n",
    "        subset = data[columns_names]\n",
    "    else:\n",
    "        subset = data\n",
    "\n",
    "    subset = subset.sample(frac=sample_size, random_state=42)  # Use a sample of the data for faster computation (VERY LONG IF FULL)\n",
    "    for num_clusters in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(subset)\n",
    "        silhouette_avg = silhouette_score(subset, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        \n",
    "    if print:   # If user wants to print the graph\n",
    "        PlotSilhouetteScore(cluster_range, silhouette_scores)\n",
    "    \n",
    "    return cluster_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "\n",
    "\n",
    "\n",
    "def PlotSilhouetteScore(cluster_range, silhouette_scores):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "    plt.title('Silhouette Score Method for Optimal Number of Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.show()\n",
    "\n",
    "def PlotClusters(data, x_col, y_col, cluster_col, title, xlabel, ylabel, palette='viridis'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=data, x=x_col, y=y_col, hue=cluster_col, palette=palette)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(title=cluster_col)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def CreateCluster(data, num_clusters, cluster_category: str, list_columns: list = None):\n",
    "    if list_columns is None:\n",
    "        list_columns = data.columns\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    data[cluster_category] = kmeans.fit_predict(data[list_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No features that have a high correlation with claim_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_profile = df[['vehicle_age', 'max_power', 'customer_age', 'region_code', 'region_density', 'subscription_length', 'claim_status',\n",
    "                   'horse_power', 'rpm']]\n",
    "correlation_matrix = risk_profile.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Diabetes Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as a base cluster that takes into account all categories\n",
    "num_clusters = CalculteOptimalClusters(risk_profile, 10, 0.25, print=True)\n",
    "CreateCluster(risk_profile, num_clusters, 'risk_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotClusters(risk_profile, 'customer_age', 'vehicle_age', 'risk_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Vehicle Age')\n",
    "\n",
    "num_clusters = CalculteOptimalClusters(risk_profile[['customer_age', 'horse_power']], 10, 0.25, print=True)\n",
    "CreateCluster(risk_profile, num_clusters, 'ca-va_cluster', ['customer_age', 'vehicle_age'])\n",
    "PlotClusters(risk_profile, 'customer_age', 'vehicle_age', 'ca-va_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Vehicle Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotClusters(risk_profile, 'customer_age', 'horse_power', 'risk_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Horse Power')\n",
    "\n",
    "num_clusters = CalculteOptimalClusters(risk_profile[['customer_age', 'horse_power']], 10, 0.25, print=True)\n",
    "CreateCluster(risk_profile, num_clusters, 'ca-hp_cluster', ['customer_age', 'horse_power'])\n",
    "PlotClusters(risk_profile, 'customer_age', 'horse_power', 'ca-hp_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Horse Power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotClusters(risk_profile, 'customer_age', 'region_code', 'risk_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Region Code')\n",
    "\n",
    "num_clusters = CalculteOptimalClusters(risk_profile[['customer_age', 'region_code']], 10, 0.25, print=True)\n",
    "CreateCluster(risk_profile, num_clusters, 'ca-rc_cluster', ['customer_age', 'region_code'])\n",
    "PlotClusters(risk_profile, 'customer_age', 'region_code', 'ca-rc_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Region Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotClusters(risk_profile, 'customer_age', 'region_density', 'risk_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Region Density')\n",
    "\n",
    "num_clusters = CalculteOptimalClusters(risk_profile[['customer_age', 'region_density']], 10, 0.25, print=True)\n",
    "CreateCluster(risk_profile, num_clusters, 'ca-rd_cluster', ['customer_age', 'region_density'])\n",
    "PlotClusters(risk_profile, 'customer_age', 'region_density', 'ca-rd_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'Region Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotClusters(risk_profile, 'customer_age', 'rpm', 'risk_cluster', 'Clusters of Risk Profiles', 'Vehicle Age', 'RPM')\n",
    "\n",
    "num_clusters = CalculteOptimalClusters(risk_profile[['customer_age', 'rpm']], 10, 0.25, print=True)\n",
    "CreateCluster(risk_profile, num_clusters, 'ca-rpm_cluster', ['customer_age', 'rpm'])\n",
    "PlotClusters(risk_profile, 'customer_age', 'rpm', 'ca-rpm_cluster', 'Clusters of Risk Profiles', 'Customer Age', 'RPM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "safety_features = [ # Features of Interest\n",
    "    'airbags',\n",
    "    'is_parking_sensors', 'is_parking_camera', 'is_front_fog_lights',\n",
    "    'is_central_locking', 'is_speed_alert', 'rear_brakes_type',\n",
    "    'model', 'transmission_type', 'steering_type', 'is_brake_assist', 'is_power_steering',\n",
    "    'ncap_rating'\n",
    "]\n",
    "\n",
    "for feature in safety_features: # Convert Yes/No to 1/0\n",
    "    data[feature] = data[feature].replace({'Yes': 1, 'No': 0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in safety_features:\n",
    "    data[feature] = data[feature].replace({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "# Result DF\n",
    "anomaly_counts = pd.DataFrame(columns=['Feature', 'Anomaly_Count'])\n",
    "\n",
    "for feature in safety_features:\n",
    "    df_subset = data[[feature, 'claim_status']].copy()  # Iterates through each feature and the target variable claim_status\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_subset)\n",
    "\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    df_subset['anomaly'] = iso_forest.fit_predict(X_scaled)\n",
    "    \n",
    "    anomaly_count = (df_subset['anomaly'] == -1).sum()  # -1 label indiciates anomalies\n",
    "    \n",
    "    new_row = pd.DataFrame({'Feature': [feature], 'Anomaly_Count': [anomaly_count]})\n",
    "    anomaly_counts = pd.concat([anomaly_counts, new_row], ignore_index=True)\n",
    "\n",
    "anomaly_counts = anomaly_counts.sort_values(by='Anomaly_Count', ascending=False)\n",
    "\n",
    "# Display the top 3 features with the highest anomalies when compared to the target variable claim_status\n",
    "top_3_features = anomaly_counts.head(3)\n",
    "print(\"Top 3 Safety Features with Highest Anomalies:\")\n",
    "print(top_3_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features = anomaly_counts.sort_values(by='Anomaly_Count', ascending=False).head(5)\n",
    "print(\"Top 5 Safety Features with Highest Anomalies:\")\n",
    "print(top_5_features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Feature', y='Anomaly_Count', data=top_5_features, palette='viridis')\n",
    "plt.title('Top 5 Safety Features with Highest Anomalies')\n",
    "plt.xlabel('Safety Feature')\n",
    "plt.ylabel('Number of Anomalies')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "data = df.copy()\n",
    "data['region_code'] = data['region_code'].astype(int)\n",
    "\n",
    "features = ['steering_type', 'is_parking_sensors', 'ncap_rating', 'model']\n",
    "data = data.dropna(subset=features + ['region_code', 'claim_status'])\n",
    "\n",
    "if data['is_parking_sensors'].dtype == object:      ## !!!!!!! REPLACE?\n",
    "    data['is_parking_sensors'] = data['is_parking_sensors'].replace({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "categorical_features = ['steering_type', 'model']\n",
    "label_encoders = {}\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[feature] = le.fit_transform(data[feature])\n",
    "    label_encoders[feature] = le\n",
    "\n",
    "data['ncap_rating'] = data['ncap_rating'].astype(float)\n",
    "\n",
    "region_details = []\n",
    "\n",
    "# Loop Through Features and Regions - Find amount of outlines per each combination\n",
    "regions = data['region_code'].unique()\n",
    "for feature in features:\n",
    "    total_outliers = 0\n",
    "    for region in regions:\n",
    "        region_data = data[data['region_code'] == region]\n",
    "        if len(region_data) < 5:\n",
    "            continue\n",
    "        X = region_data[[feature, 'claim_status']]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        labels = dbscan.fit_predict(X_scaled)\n",
    "        num_outliers = np.sum(labels == -1)\n",
    "        total_outliers += num_outliers\n",
    "        region_details.append({'Region': region, 'Feature': feature, 'Outliers': num_outliers})\n",
    "\n",
    "\n",
    "region_feature_outliers = pd.DataFrame(region_details)\n",
    "top_10_region_features = region_feature_outliers.sort_values(by='Outliers', ascending=False).head(10)\n",
    "print(\"Top 10 Region-Feature Relationships with Highest Outliers:\")\n",
    "print(top_10_region_features)\n",
    "\n",
    "\n",
    "# Step 5: Visualize Results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Region', y='Outliers', hue='Feature', data=top_10_region_features, palette='viridis')\n",
    "plt.title('Top 10 Region-Feature Relationships with Highest Outliers')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Number of Outliers')\n",
    "plt.legend(title='Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import shap\n",
    "\n",
    "# Select features for the model\n",
    "features = ['vehicle_age', 'customer_age', 'subscription_length', 'max_torque',\n",
    "           'max_power', 'displacement', 'turning_radius', 'length', 'width', \n",
    "           'gross_weight', 'region_code', 'region_density']\n",
    "\n",
    "X = df[features]\n",
    "y = df['claim_status']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create a balanced dataset\n",
    "sampler = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = sampler.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Train the model with balanced data\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train with balanced data\n",
    "history = model.fit(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Calculate SHAP values for feature importance\n",
    "explainer = shap.DeepExplainer(model, X_train_scaled[:100])\n",
    "shap_values = explainer.shap_values(X_train_scaled[:100])\n",
    "\n",
    "# Calculate feature importance with proper reshaping\n",
    "feature_importance = np.abs(shap_values[0]).mean(0)\n",
    "feature_importance = feature_importance * np.ones(len(X_train.columns))\n",
    "\n",
    "# Create DataFrame with lists instead of arrays\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': list(X_train.columns),\n",
    "    'Importance': list(feature_importance)\n",
    "})\n",
    "\n",
    "# Sort values\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=feature_importance_df,\n",
    "    x='Importance',\n",
    "    y='Feature'\n",
    ")\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.xlabel('Mean |SHAP value|')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Evaluate model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, confusion_matrix\n",
    "\n",
    "# Get model predictions on test set\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'PR curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC Score\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Binning\n",
    "df['region_density_bin'] = pd.qcut(df['region_density'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "df['horse_power_bin'] = pd.qcut(df['horse_power'], q=3, labels=[\"LowHP\", \"MedHP\", \"HighHP\"])\n",
    "df['rpm_bin'] = pd.qcut(df['rpm'], q=3, labels=[\"LowRPM\", \"HighRPM\"], duplicates='drop')\n",
    "df['displacement_bin'] = pd.qcut(df['displacement'], q=3, labels=[\"SmallDisp\", \"MedDisp\", \"LargeDisp\"])\n",
    "\n",
    "cols_of_interest = [\n",
    "    'region_density_bin', 'model', 'rear_brakes_type', \n",
    "    'transmission_type', 'horse_power_bin', 'rpm_bin', \n",
    "    'ncap_rating', 'displacement_bin', 'claim_status'\n",
    "]\n",
    "df_sub = df[cols_of_interest]\n",
    "\n",
    "df_encoded = pd.get_dummies(df_sub, columns=[\n",
    "    'region_density_bin', 'model', 'rear_brakes_type', \n",
    "    'transmission_type', 'horse_power_bin', 'rpm_bin', \n",
    "    'ncap_rating', 'displacement_bin', 'claim_status'\n",
    "])\n",
    "\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Rules\n",
    "lift_rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "confidence_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "support_rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01)\n",
    "\n",
    "lift_rules['antecedents'] = lift_rules['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "lift_rules['consequents'] = lift_rules['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "\n",
    "# Top 10 Rules by Lift\n",
    "top_lift = lift_rules.sort_values(by='lift', ascending=False).head(10)\n",
    "print(\"Top 10 Rules by Lift:\")\n",
    "display(top_lift[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "# Top 10 Rules by Confidence\n",
    "top_confidence = confidence_rules.sort_values(by='confidence', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Rules by Confidence:\")\n",
    "display(top_confidence[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "# Top 10 Rules by Support\n",
    "top_support = support_rules.sort_values(by='support', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Rules by Support:\")\n",
    "display(top_support[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sequential Pattern Analysis   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_claim_rate = df.groupby('subscription_length')['claim_status'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_claim_rate['subscription_length'], avg_claim_rate['claim_status'], marker='o')\n",
    "plt.xlabel('Subscription Length (Years)')\n",
    "plt.ylabel('Average Claim Rate')\n",
    "plt.title('Claim Pattern by Subscription Length')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "df['subscription_length_bin'] = pd.cut(df['subscription_length'], bins=[0,1,2,3,4,5], labels=['0-1','1-2','2-3','3-4','4-5'])\n",
    "avg_claim_rate_bin = df.groupby('subscription_length_bin')['claim_status'].mean().reset_index()\n",
    "\n",
    "plt.bar(avg_claim_rate_bin['subscription_length_bin'], avg_claim_rate_bin['claim_status'])\n",
    "plt.xlabel('Subscription Length (Years)')\n",
    "plt.ylabel('Average Claim Rate')\n",
    "plt.title('Claim Pattern by Binned Subscription Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-Risk Feature Combinations\n",
    "Taken from Analaysis of Association Rules with Respect to Claim Status\n",
    "By Support:\n",
    "    rear_brakes_type_1\t& transmission_type_1\t0.651642\t\n",
    "    transmission_type_1\trear_brakes_type_1\t0.651642\t\n",
    "    rear_brakes_type_1\trpm_bin_HighRPM\t0.626178\n",
    "    transmission_type_1\trear_brakes_type_1\t0.651642\t\n",
    "    rear_brakes_type_1\trpm_bin_HighRPM\t0.626178\t\n",
    "    rpm_bin_HighRPM\trear_brakes_type_1\t0.626178"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
